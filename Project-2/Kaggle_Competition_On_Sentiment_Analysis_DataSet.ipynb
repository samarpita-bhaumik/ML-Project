{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-15T17:27:34.545715Z","iopub.status.busy":"2023-12-15T17:27:34.545297Z","iopub.status.idle":"2023-12-15T17:27:35.516731Z","shell.execute_reply":"2023-12-15T17:27:35.515720Z","shell.execute_reply.started":"2023-12-15T17:27:34.545681Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/dataset-preprocessed/preprocessed_data.csv\n","/kaggle/input/ratemeter/sample_submission.csv\n","/kaggle/input/ratemeter/train.csv\n","/kaggle/input/ratemeter/test.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:27:42.357475Z","iopub.status.busy":"2023-12-15T17:27:42.356470Z","iopub.status.idle":"2023-12-15T17:27:54.823156Z","shell.execute_reply":"2023-12-15T17:27:54.822028Z","shell.execute_reply.started":"2023-12-15T17:27:42.357440Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:28:06.118286Z","iopub.status.busy":"2023-12-15T17:28:06.117867Z","iopub.status.idle":"2023-12-15T17:28:22.712276Z","shell.execute_reply":"2023-12-15T17:28:22.711239Z","shell.execute_reply.started":"2023-12-15T17:28:06.118249Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /kaggle/working/...\n","[nltk_data]   Package wordnet is already up-to-date!\n","Archive:  /kaggle/working/corpora/wordnet.zip\n"]},{"name":"stderr","output_type":"stream","text":["replace /kaggle/working/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n","(EOF or read error, treating as \"[N]one\" ...)\n"]},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 400000 entries, 0 to 399999\n","Data columns (total 12 columns):\n"," #   Column                    Non-Null Count   Dtype \n","---  ------                    --------------   ----- \n"," 0   user_id                   400000 non-null  object\n"," 1   book_id                   400000 non-null  int64 \n"," 2   review_id                 400000 non-null  object\n"," 3   review_text               400000 non-null  object\n"," 4   date_added                400000 non-null  object\n"," 5   date_updated              400000 non-null  object\n"," 6   read_at                   359053 non-null  object\n"," 7   started_at                277877 non-null  object\n"," 8   n_votes                   400000 non-null  int64 \n"," 9   n_comments                400000 non-null  int64 \n"," 10  rating                    400000 non-null  int64 \n"," 11  preprocessed_review_text  399883 non-null  object\n","dtypes: int64(4), object(8)\n","memory usage: 36.6+ MB\n"]}],"source":["import pandas as pd\n","import re\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","\n","#Download NLTK resources\n","import nltk\n","import subprocess\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","try:\n","    nltk.data.find('wordnet.zip')\n","except:\n","    nltk.download('wordnet', download_dir='/kaggle/working/')\n","    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n","    subprocess.run(command.split())\n","    nltk.data.path.append('/kaggle/working/')\n","\n","# Load the dataset\n","# Assuming you have a CSV file with 'text' and 'rating' columns\n","# dataset = pd.read_csv(r\"/kaggle/input/ratemeter/train.csv\")\n","# dataset = dataset.iloc[100000:400000].copy()\n","\n","# Function for text preprocessing\n","def preprocess_text(text):\n","    # Remove HTML tags\n","    text = re.sub(r'<.*?>', '', text)\n","    \n","    # Remove special characters and numbers\n","    text = re.sub(r'[^a-zA-Z]', ' ', text)\n","    \n","    # Convert to lowercase\n","    text = text.lower()\n","    \n","    # Tokenization\n","    tokens = word_tokenize(text)\n","    \n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","    \n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","    \n","    # Join tokens back into a string\n","    preprocessed_text = ' '.join(tokens)\n","    \n","    return preprocessed_text\n","\n","# Apply preprocessing to the 'text' column\n","# dataset['preprocessed_review_text'] = dataset['review_text'].apply(preprocess_text)\n","\n","# Save DataFrame to CSV\n","# dataset.to_csv('preprocessed_data.csv', index=False)\n","\n","# # Display the preprocessed data\n","dataset = pd.read_csv(r\"/kaggle/input/dataset-preprocessed/preprocessed_data.csv\")\n","# dataset = dataset.iloc[:400000].copy()\n","# dataset = dataset.sample(frac=1).reset_index(drop=True)\n","# print(dataset[['review_text', 'preprocessed_review_text', 'rating']].head())\n","dataset.info()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:29:06.608819Z","iopub.status.busy":"2023-12-15T17:29:06.608385Z","iopub.status.idle":"2023-12-15T17:33:14.162217Z","shell.execute_reply":"2023-12-15T17:33:14.160598Z","shell.execute_reply.started":"2023-12-15T17:29:06.608786Z"},"trusted":true},"outputs":[],"source":["# # Feature Extraction using TF-IDF\n","# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","# vectorizer_tfidf = TfidfVectorizer()\n","columns_to_exclude = ['rating', 'review_id','date_added','date_updated','read_at','started_at']\n","dataset['combined_text'] = dataset.drop(columns_to_exclude, axis=1).astype('str').apply(lambda x: ' '.join(x), axis=1)\n","\n","# dataset['combined_text'] = dataset.drop('rating', axis=1).astype('str').apply(lambda x: ' '.join(x), axis=1)\n","# # dataset['combined_text'] = dataset.astype('str').apply(lambda x: ' '.join(x), axis=1)\n","# X_tfidf = vectorizer_tfidf.fit_transform(dataset['combined_text'].values.astype('str'))\n","# print(X_tfidf.toarray())\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import scipy.sparse\n","\n","# Feature Extraction using TF-IDF\n","vectorizer_tfidf = TfidfVectorizer(max_features=5000)  # Limiting vocabulary size for memory efficiency\n","\n","# Fit the vectorizer on the entire dataset\n","vectorizer_tfidf.fit(dataset['combined_text'].astype('str'))\n","\n","# Process data in smaller batches to reduce memory consumption\n","batch_size = 1000\n","num_batches = len(dataset) // batch_size + (1 if len(dataset) % batch_size != 0 else 0)\n","\n","# Initialize an empty list to store sparse matrices\n","X_tfidf_batches = []\n","\n","for i in range(num_batches):\n","    start_idx = i * batch_size\n","    end_idx = min((i + 1) * batch_size, len(dataset))\n","    \n","    # Use sparse matrix representation\n","    X_tfidf_batch = vectorizer_tfidf.transform(dataset['combined_text'].iloc[start_idx:end_idx].astype('str'))\n","    \n","    # Append the batch to the list\n","    if X_tfidf_batch.shape[0] > 0:\n","        X_tfidf_batches.append(X_tfidf_batch)\n","\n","# Concatenate the batches vertically if there are any\n","if X_tfidf_batches:\n","    X_tfidf_sparse = scipy.sparse.vstack(X_tfidf_batches)\n","else:\n","    # Handle the case when the dataset is empty\n","    X_tfidf_sparse = scipy.sparse.csr_matrix((0, len(vectorizer_tfidf.get_feature_names_out())), dtype=float)\n","\n","# Optionally, convert to a dense array for further processing or display\n","X_tfidf_dense = X_tfidf_sparse.toarray()\n","\n","# Your code continues here...\n","# (e.g., model training, evaluation, etc.)\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:22:59.689720Z","iopub.status.busy":"2023-12-15T11:22:59.689325Z","iopub.status.idle":"2023-12-15T11:22:59.698115Z","shell.execute_reply":"2023-12-15T11:22:59.697415Z","shell.execute_reply.started":"2023-12-15T11:22:59.689687Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(400000, 638343)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["X_tfidf.shape"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:23:03.332493Z","iopub.status.busy":"2023-12-15T11:23:03.332079Z","iopub.status.idle":"2023-12-15T11:23:24.306787Z","shell.execute_reply":"2023-12-15T11:23:24.305617Z","shell.execute_reply.started":"2023-12-15T11:23:03.332456Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting xgboost\n","  Downloading xgboost-2.0.2-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from xgboost) (1.11.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from xgboost) (1.26.2)\n","Installing collected packages: xgboost\n","Successfully installed xgboost-2.0.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install xgboost"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:42:06.178229Z","iopub.status.busy":"2023-12-15T17:42:06.177955Z","iopub.status.idle":"2023-12-15T17:43:26.094989Z","shell.execute_reply":"2023-12-15T17:43:26.094011Z","shell.execute_reply.started":"2023-12-15T17:42:06.178204Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"]},{"name":"stdout","output_type":"stream","text":["F1 Score: 0.5245196495416039\n","\n","Confusion Matrix:\n"," [[  804   155   212   303   754   539]\n"," [  148   672   859   393   301   177]\n"," [  128   305  1935  2593  1257   282]\n"," [  140    95  1063  7092  7402  1075]\n"," [  121    59   229  3692 17409  6287]\n"," [  118    39    84   610  8080 14588]]\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype):\n","/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"]}],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import SGDClassifier\n","# from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_tfidf_sparse, dataset['rating'], test_size=0.2, random_state=42)\n","\n","from sklearn.feature_selection import SelectFromModel\n","\n","# Fit a logistic regression model for feature selection\n","model = LogisticRegression()\n","selector = SelectFromModel(estimator=model)\n","X_train_selected = selector.fit_transform(X_train, y_train)\n","X_test_selected = selector.transform(X_test)\n","\n","# Random Forest Model\n","# rf_model = RandomForestClassifier(n_estimators=50, random_state=42)\n","# rf_model.fit(X_train, y_train)\n","# rf_pred = rf_model.predict(X_test)\n","\n","# XGBoost Model\n","# xgb_model = XGBClassifier(n_estimators=100, random_state=42)\n","# xgb_model.fit(X_train, y_train)\n","# xgb_pred = xgb_model.predict(X_test)\n","\n","# Logistic Regression Model\n","log_model = LogisticRegression()\n","\n","# param_grid = {'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100], 'penalty': ['l1', 'l2']}\n","# grid_search = GridSearchCV(log_model, param_grid, cv=5, scoring='accuracy')\n","# param_grid = {'C': [0.4], 'penalty': ['l2']}\n","# grid_search = GridSearchCV(log_model, param_grid, cv=5, scoring='accuracy')\n","# grid_search.fit(X_train, y_train)\n","# best_params = grid_search.best_params_\n","# print(\"Best Hyperparameters:\", best_params)\n","# best_model = grid_search.best_estimator_\n","# test_accuracy = best_model.score(X_test, y_test)\n","# print(\"Test Accuracy:\", test_accuracy)\n","# log_pred = best_model.predict(X_test)\n","log_model.fit(X_train_selected, y_train)\n","log_pred = log_model.predict(X_test_selected)\n","\n","# Stochastic Gradient Descent Model\n","# sgd_model = SGDClassifier()\n","# sgd_model.fit(X_train, y_train)\n","# sgd_predictions = sgd_model.predict(X_test)\n","\n","# Linear SVC\n","# svm_model = LinearSVC()\n","# svm_model.fit(X_train, y_train)\n","# svm_predictions = svm_model.predict(X_test)\n","\n","# Multinomial Naive Bayes Model\n","# nb_model = MultinomialNB()\n","# nb_model.fit(X_train, y_train)\n","\n","# # Make predictions\n","# nb_predictions = nb_model.predict(X_test)\n","\n","\n","# Non Linear SVC\n","# Standardize the features (important for SVM)\n","# scaler = StandardScaler(with_mean=False)\n","# X_train_std = scaler.fit_transform(X_train)\n","# X_test_std = scaler.transform(X_test)\n","# # Train the SVM model\n","# svm_model = SVC(kernel='rbf', C=1.0, random_state=42)\n","# svm_model.fit(X_train_std, y_train)\n","\n","# # Make predictions\n","# svm_predictions = svm_model.predict(X_test_std)\n","\n","\n","# Ensemble Predictions\n","# rf_pred = rf_model.predict(X_test)\n","# xgb_pred = xgb_model.predict(X_test)\n","# log_pred = log_model.predict(X_test)\n","\n","# # Combine predictions using majority voting\n","# ensemble_pred = (rf_pred + xgb_pred + log_pred) // 3  # You can also experiment with different combination strategies\n","\n","print(\"F1 Score:\", f1_score(y_test, log_pred, average='weighted'))\n","print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, log_pred))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:44:14.695361Z","iopub.status.busy":"2023-12-15T17:44:14.694441Z","iopub.status.idle":"2023-12-15T17:44:21.547689Z","shell.execute_reply":"2023-12-15T17:44:21.546746Z","shell.execute_reply.started":"2023-12-15T17:44:14.695327Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>book_id</th>\n","      <th>review_id</th>\n","      <th>review_text</th>\n","      <th>date_added</th>\n","      <th>date_updated</th>\n","      <th>read_at</th>\n","      <th>started_at</th>\n","      <th>n_votes</th>\n","      <th>n_comments</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>de3a6a28c83cda006b415d45d64674c9</td>\n","      <td>1618</td>\n","      <td>d76ce5becf493e5c653610edb806ffc4</td>\n","      <td>I'm going to keep this review short, because I...</td>\n","      <td>Tue Jun 09 10:37:48 -0700 2015</td>\n","      <td>Wed Jun 10 13:43:38 -0700 2015</td>\n","      <td>Wed Jun 10 14:08:39 -0700 2015</td>\n","      <td>Tue Jun 09 00:00:00 -0700 2015</td>\n","      <td>8</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>d9cfab35a87e32084b1817dfb0e98748</td>\n","      <td>15776309</td>\n","      <td>9c5c9aed79255a1a610dfc153ee90ad6</td>\n","      <td>You know, I was really stoked to see this come...</td>\n","      <td>Mon Apr 22 09:19:40 -0700 2013</td>\n","      <td>Wed Jul 10 14:34:27 -0700 2013</td>\n","      <td>Wed Jul 10 14:34:27 -0700 2013</td>\n","      <td>Tue Jul 09 00:00:00 -0700 2013</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>24935a870a46525a37be92775ab18f76</td>\n","      <td>478927</td>\n","      <td>ad26332459cda8f40581fef7a29b800c</td>\n","      <td>This is one of those books where you know you ...</td>\n","      <td>Sat Mar 14 12:16:55 -0700 2015</td>\n","      <td>Sun Apr 05 11:25:00 -0700 2015</td>\n","      <td>Wed Mar 18 00:00:00 -0700 2015</td>\n","      <td>Sat Mar 14 00:00:00 -0700 2015</td>\n","      <td>14</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6ccb40eabdd0db7895aac00963256469</td>\n","      <td>22628</td>\n","      <td>5bb75768be7f2ddcd632336778b67f5a</td>\n","      <td>The perks of being a wallflower \\n What does a...</td>\n","      <td>Sat Sep 12 08:43:30 -0700 2015</td>\n","      <td>Sat Sep 12 09:19:29 -0700 2015</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9f9d0f6e9a6a5797a252ef81abc9421c</td>\n","      <td>13596809</td>\n","      <td>3d7f76ea566b9ce0700772236094d936</td>\n","      <td>So, I wrote a review for this when I read it b...</td>\n","      <td>Sun Jul 14 19:36:13 -0700 2013</td>\n","      <td>Thu Jun 02 16:45:45 -0700 2016</td>\n","      <td>Sat Mar 23 00:00:00 -0700 2013</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            user_id   book_id  \\\n","0  de3a6a28c83cda006b415d45d64674c9      1618   \n","1  d9cfab35a87e32084b1817dfb0e98748  15776309   \n","2  24935a870a46525a37be92775ab18f76    478927   \n","3  6ccb40eabdd0db7895aac00963256469     22628   \n","4  9f9d0f6e9a6a5797a252ef81abc9421c  13596809   \n","\n","                          review_id  \\\n","0  d76ce5becf493e5c653610edb806ffc4   \n","1  9c5c9aed79255a1a610dfc153ee90ad6   \n","2  ad26332459cda8f40581fef7a29b800c   \n","3  5bb75768be7f2ddcd632336778b67f5a   \n","4  3d7f76ea566b9ce0700772236094d936   \n","\n","                                         review_text  \\\n","0  I'm going to keep this review short, because I...   \n","1  You know, I was really stoked to see this come...   \n","2  This is one of those books where you know you ...   \n","3  The perks of being a wallflower \\n What does a...   \n","4  So, I wrote a review for this when I read it b...   \n","\n","                       date_added                    date_updated  \\\n","0  Tue Jun 09 10:37:48 -0700 2015  Wed Jun 10 13:43:38 -0700 2015   \n","1  Mon Apr 22 09:19:40 -0700 2013  Wed Jul 10 14:34:27 -0700 2013   \n","2  Sat Mar 14 12:16:55 -0700 2015  Sun Apr 05 11:25:00 -0700 2015   \n","3  Sat Sep 12 08:43:30 -0700 2015  Sat Sep 12 09:19:29 -0700 2015   \n","4  Sun Jul 14 19:36:13 -0700 2013  Thu Jun 02 16:45:45 -0700 2016   \n","\n","                          read_at                      started_at  n_votes  \\\n","0  Wed Jun 10 14:08:39 -0700 2015  Tue Jun 09 00:00:00 -0700 2015        8   \n","1  Wed Jul 10 14:34:27 -0700 2013  Tue Jul 09 00:00:00 -0700 2013        0   \n","2  Wed Mar 18 00:00:00 -0700 2015  Sat Mar 14 00:00:00 -0700 2015       14   \n","3                             NaN                             NaN        0   \n","4  Sat Mar 23 00:00:00 -0700 2013                             NaN        0   \n","\n","   n_comments  \n","0           0  \n","1           0  \n","2           0  \n","3           0  \n","4           0  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["#Now checking the model on test data\n","dataset_test=pd.read_csv(r\"/kaggle/input/ratemeter/test.csv\")\n","# data_test = dataset_test.iloc[:10000].copy()\n","dataset_test.head()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T12:39:18.886458Z","iopub.status.busy":"2023-12-15T12:39:18.886083Z","iopub.status.idle":"2023-12-15T12:39:19.008350Z","shell.execute_reply":"2023-12-15T12:39:19.007586Z","shell.execute_reply.started":"2023-12-15T12:39:18.886427Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 270000 entries, 0 to 269999\n","Data columns (total 10 columns):\n"," #   Column        Non-Null Count   Dtype \n","---  ------        --------------   ----- \n"," 0   user_id       270000 non-null  object\n"," 1   book_id       270000 non-null  int64 \n"," 2   review_id     270000 non-null  object\n"," 3   review_text   270000 non-null  object\n"," 4   date_added    270000 non-null  object\n"," 5   date_updated  270000 non-null  object\n"," 6   read_at       242459 non-null  object\n"," 7   started_at    187740 non-null  object\n"," 8   n_votes       270000 non-null  int64 \n"," 9   n_comments    270000 non-null  int64 \n","dtypes: int64(3), object(7)\n","memory usage: 20.6+ MB\n"]}],"source":["dataset_test.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Apply preprocessing to the 'text' column\n","dataset_test['preprocessed_review_text'] = dataset_test['review_text'].apply(preprocess_text)\n","\n","# Display the preprocessed data\n","print(dataset_test[['review_text', 'preprocessed_review_text']].head())"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T12:56:32.649350Z","iopub.status.busy":"2023-12-15T12:56:32.649077Z","iopub.status.idle":"2023-12-15T13:04:40.254078Z","shell.execute_reply":"2023-12-15T13:04:40.253196Z","shell.execute_reply.started":"2023-12-15T12:56:32.649324Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.02752735 0.         0.         ... 0.         0.         0.        ]\n"," [0.03281758 0.         0.         ... 0.         0.         0.        ]\n"," [0.0784773  0.         0.         ... 0.         0.         0.        ]\n"," ...\n"," [0.06800149 0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.18446649 0.         0.         ... 0.         0.         0.        ]]\n"]}],"source":["# columns_to_exclude = ['review_id','date_added','date_updated','read_at','started_at']\n","# dataset_test['combined_text'] = dataset_test.drop(columns_to_exclude, axis=1).astype('str').apply(lambda x: ' '.join(x), axis=1)\n","# dataset_test['combined_text'] = dataset_test.astype('str').apply(lambda x: ' '.join(x), axis=1)\n","# X_tfidf_test = vectorizer_tfidf.transform(dataset_test['combined_text'].values.astype('str'))\n","# print(X_tfidf_test.toarray())\n","\n","columns_to_exclude = ['rating', 'review_id','date_added','date_updated','read_at','started_at']\n","dataset_test['combined_text'] = dataset.drop(columns_to_exclude, axis=1).astype('str').apply(lambda x: ' '.join(x), axis=1)\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import scipy.sparse\n","\n","# Feature Extraction using TF-IDF\n","vectorizer_tfidf = TfidfVectorizer(max_features=5000)  # Limiting vocabulary size for memory efficiency\n","\n","# Fit the vectorizer on the entire dataset\n","vectorizer_tfidf.fit(dataset_test['combined_text'].astype('str'))\n","\n","# Process data in smaller batches to reduce memory consumption\n","batch_size = 1000\n","num_batches = len(dataset_test) // batch_size + (1 if len(dataset_test) % batch_size != 0 else 0)\n","\n","# Initialize an empty list to store sparse matrices\n","X_tfidf_batches = []\n","\n","for i in range(num_batches):\n","    start_idx = i * batch_size\n","    end_idx = min((i + 1) * batch_size, len(dataset_test))\n","    \n","    # Use sparse matrix representation\n","    X_tfidf_batch = vectorizer_tfidf.transform(dataset_test['combined_text'].iloc[start_idx:end_idx].astype('str'))\n","    \n","    # Append the batch to the list\n","    if X_tfidf_batch.shape[0] > 0:\n","        X_tfidf_batches.append(X_tfidf_batch)\n","\n","# Concatenate the batches vertically if there are any\n","if X_tfidf_batches:\n","    X_tfidf_sparse_test = scipy.sparse.vstack(X_tfidf_batches)\n","else:\n","    # Handle the case when the dataset is empty\n","    X_tfidf_sparse_test = scipy.sparse.csr_matrix((0, len(vectorizer_tfidf.get_feature_names_out())), dtype=float)\n","\n","# Optionally, convert to a dense array for further processing or display\n","# X_tfidf_dense_test = X_tfidf_sparse.toarray()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T13:04:46.206837Z","iopub.status.busy":"2023-12-15T13:04:46.206066Z","iopub.status.idle":"2023-12-15T13:04:46.491345Z","shell.execute_reply":"2023-12-15T13:04:46.490522Z","shell.execute_reply.started":"2023-12-15T13:04:46.206754Z"},"trusted":true},"outputs":[],"source":["# Ensemble Predictions\n","# rf_pred_test = rf_model.predict(X_tfidf_test)\n","# xgb_pred_test = xgb_model.predict(X_tfidf_test)\n","log_pred_test = best_model.predict(X_tfidf_sparse_test)\n","\n","# log_pred_test = rf_model.predict(X_tfidf_test)\n","\n","# # Combine predictions using majority voting\n","# ensemble_pred = (rf_pred + xgb_pred + log_pred) // 3"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T13:04:49.909353Z","iopub.status.busy":"2023-12-15T13:04:49.908477Z","iopub.status.idle":"2023-12-15T13:04:50.304139Z","shell.execute_reply":"2023-12-15T13:04:50.303222Z","shell.execute_reply.started":"2023-12-15T13:04:49.909311Z"},"trusted":true},"outputs":[],"source":["output = pd.DataFrame({'review_id': dataset_test['review_id'].values,\n","                      'rating': log_pred_test})\n","output.to_csv('submission11.csv',index=False, header=True)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":6953956,"sourceId":63658,"sourceType":"competition"},{"datasetId":4155962,"sourceId":7188327,"sourceType":"datasetVersion"},{"datasetId":4160169,"sourceId":7193853,"sourceType":"datasetVersion"},{"datasetId":4162660,"sourceId":7197516,"sourceType":"datasetVersion"}],"dockerImageVersionId":30615,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
